\section{Two-player zero-sum games}

\subsection{Minimax and maximin}

\label{sec:TPZSgames}

Consider a zero-sum game determined by the matrix $\vect{A} \in \mathbb{R}^{m_1
\times m_2}_{\ge 0}$ which gives the payoffs to player I. Since the game is
zero-sum means that the payoff matrix for player II $\vect{B} = -\vect{A}$.

\begin{definition}[Maximin value]
	The maximin value is the highest payoff a player can be sure to receive
	without knowing the actions of the other players. Equivalently, this is the
	lowest value other players can force the player to receive when they know
	the player's action:

	\begin{equation}
		\underline{v_i} = \max_{s_i} \min_{s_{-i}} u_i(s_i, s_{-i})
	\end{equation}
\end{definition}

\begin{definition}[Minimax value]
	The minimax value is the smallest value the other players can force the
	player to receive, without knowing the player's actions. Equivalently, it
	is the largest value the player can be sure to get when they know the
	actions of the other players:

	\begin{equation}
		\overline{v_i} = \min_{s_{-i}} \max_{s_i} u_i(s_i, s_{-i})
	\end{equation}
\end{definition}

Consider the maximin values of the players in a two-player zero-sum game. Since
$u_\text{I}+u_\text{II}=0$, we can denote $u$ as the payoff to player I and
$-u$ as the payoff to player II. Player I's maximin is:
\begin{equation*}
	\underline{v_\text{I}} = \max_{s_\text{I} \in S_\text{I}} \min_{s_\text{II}
	\in S_\text{II}} u(s_\text{I}, s_\text{II})
\end{equation*}

Player II's maximin value is:
\begin{equation*}
		\underline{v_\text{II}} = \max_{s_\text{II} \in S_\text{II}}
		\min_{s_\text{I} \in S_\text{I}} -u(s_\text{I}, s_\text{II}) = -
		\min_{s_\text{II} \in S_\text{II}} \max_{s_\text{I} \in S_\text{I}}
		u(s_\text{I}, s_\text{II})
\end{equation*}

\begin{definition}[Maximin and minimax value in two-player zero-sum games]
	The \emph{maximin value} $\underline{v}$ is the minimum that player I can
	guarantee themselves to get. The \emph{minimax value} $\overline{v}$ is the
	maximum that player II can guarantee they will pay to player I:
	\begin{equation}
		\begin{split}
			\underline{v} & = \max_{s_\text{I} \in S_\text{I}} \min_{s_\text{II}
			\in S_\text{II}} u(s_\text{I}, s_\text{II}) \\
			\overline{v} & = \min_{s_\text{II} \in S_\text{II}} \max_{s_\text{I}
			\in S_\text{I}} u(s_\text{I}, s_\text{II}) \\
		\end{split}
	\end{equation}
\end{definition}

In other words:
\begin{itemize}
	\item maximin value: ``maximum of row minima''
	\item minimax value: ``minimum of column maxima''
\end{itemize}

\textbf{Example}: Consider the following zero-sum game:
\begin{center}
	\begin{tabular}{c|cc}
		& L & R \\ \hline
		T & -2 & 5 \\
		B & 3 & 0 \\
	\end{tabular}
\end{center}

We have:
\begin{equation*}
	\begin{split}
		\underline{v} & = \max \{-2,0\} = 0 \\
		\overline{v} & = \min \{3,5\} = 3
	\end{split}
\end{equation*}

The maximin strategy for player I is the strategy guaranteeing the maximin
value for player I, while the minimax strategy for player II is the strategy
guaranteeing the minimax value for player II. In the above example, player I's
maximin strategy is $B$ and player II's minimax strategy is $L$.

The minimax and maximin values may be equal or, as the example indicates,
different. It must however be the case that $\underline{v} \le \overline{v}$,
since the highest amount that player I can get must be at most the most that
player II will ever pay out to player I.

\begin{fact}
	Maximin value is at most the minimax value: $\underline{v} \le
	\overline{v}$
\end{fact}

\begin{definition}[Value of a game]
	A two-player game has a \emph{value} if $\overline{v}=\underline{v}$. In
	this case the quantity $v:=\overline{v}=\underline{v}$ is called the
	\emph{value of the game}. Any maximin and minimax strategies of player I
	and player II respectively are called \emph{optimal strategies}.
\end{definition}

\begin{definition}[Maximin]
	A strategy $\vect{x}^* \in \Delta^{m_1}$ for player I is a \textit{maximin}
	strategy if

	\begin{equation}
		\vect{x}^* \in \argmax_{\vect{x} \in \Delta^{m_1}} \left[
			\min_{j \in [m_2]} (\vect{x}^\top \vect{A})_j \right]
	\end{equation}
\end{definition}

\begin{definition}[Minimax]
	A strategy $\vect{y}^* \in \Delta^{m_2}$ for player II is a
	\textit{minimax} strategy if

	\begin{equation}
		\vect{y}^* \in \argmin_{\vect{y} \in \Delta^{m_2}} \left[
			\max_{i \in [m_1]} (\vect{A} \vect{y})_i \right]
	\end{equation}
\end{definition}

A maximin strategy ``maximizes the minimum expected payoff to player I''. A
minimax strategy ``minimises the maximum expected payoff to player II''.
Maximin and minimax values can be thought of as \textit{safety utilities} -- no
matter what the other plays, they can set a minimum payoff they will receive.

\begin{itemize}
	\item \textsc{maximin} -- utility that player I can get if he commits to
		move first
	\item \textsc{minimax} -- utility that player II can limit player I's
		utility to if player II commits to move first
\end{itemize}

\begin{theorem}[von Neumann, 1928]
	Let $X \subset \mathbb{R}^n, Y \subset \mathbb{R}^m$ be compact convex
	sets. If $f:X\times Y \rightarrow \mathbb{R}$ is a continuous function that
	is concave-convex, that is:
	\begin{equation*}
		\begin{split}
			& f(\cdot,y): X \rightarrow \mathbb{R} \text{ is concave for fixed } y \\
			& f(x,\cdot): Y \rightarrow \mathbb{R} \text{ is convex for fixed } x \\
		\end{split}
	\end{equation*}

	Then we have that:
	\begin{equation}
		\max_{x \in X} \min_{y \in Y} f(x,y) = \min_{y \in Y} \max_{x \in X}
		f(x,y)
	\end{equation}
\end{theorem}

\subsection{Two-player games as linear programs}

The maximization problem is as follows:

\begin{equation}
	\begin{split}
		\text{maximise}_{\vect{x},v} v \text{ subject to } \vect{x} & \in \Delta^{m_1} \\
		\vect{x}^\top \vect{A} & \ge \begin{pmatrix}
			v \\
			\vdots \\
			v
		\end{pmatrix}
	\end{split}
\end{equation}

Note that this is not yet a Linear Program due to the first constraint. We can
therefore rewrite the optimisation problem as an LP in standard form as:

\begin{equation}
	\begin{split}
		\text{maximise}_{\vect{x},v} v \text{ subject to }
		-\vect{x}^\top \vect{A} + \begin{pmatrix}
			v \\
			\vdots \\
			v
		\end{pmatrix} & \le 0 \\
		\mathds{1}^\top \vect{x} & \le 1 \\
		\vect{x} & \ge 0
	\end{split}
\end{equation}

The primal LP formulation is the following:

\begin{equation}
	\begin{split}
		\text{maximise}_{\vect{x},v}
		(0,\ldots,0,1) \begin{pmatrix}
			x_1 \\
			\vdots \\
			x_{m_1} \\
			v
		\end{pmatrix}
		\text{ subject to }
		\begin{pmatrix}
			& & & 1 \\
			& & & \vdots  \\
			\multicolumn{3}{c}
			{\raisebox{\dimexpr\normalbaselineskip+.7\ht\strutbox-.5\height}[0pt][0pt]
			{{$-\vect{A}^\top$}}} & 1 \\
			1 & \cdots & 1 & 0
		\end{pmatrix} \begin{pmatrix}
			x_1 \\
			\vdots \\
			x_{m_1} \\
			v
		\end{pmatrix} & \le \begin{pmatrix}
			0 \\
			\vdots \\
			0 \\
			1
		\end{pmatrix} \\
		\begin{pmatrix}
			x_1 \\
			\vdots \\
			x_{m_1} \\
			v
		\end{pmatrix} & \ge 0
	\end{split} 
\end{equation}

The dual LP is:

\begin{equation}
	\begin{split}
		\text{minimise}_{\vect{y},w}
		(0,\ldots,0,1) \begin{pmatrix}
			y_1 \\
			\vdots \\
			y_{m_2} \\
			w
		\end{pmatrix}
		\text{ subject to }
		\begin{pmatrix}
			& & & 1 \\
			& & & \vdots  \\
			\multicolumn{3}{c}
			{\raisebox{\dimexpr\normalbaselineskip+.7\ht\strutbox-.5\height}[0pt][0pt]
			{{$-\vect{A}$}}} & 1 \\
			1 & \cdots & 1 & 0
		\end{pmatrix} \begin{pmatrix}
			y_1 \\
			\vdots \\
			y_{m_2} \\
			w
		\end{pmatrix} & \ge \begin{pmatrix}
			0 \\
			\vdots \\
			0 \\
			1
		\end{pmatrix} \\
		\begin{pmatrix}
			y_1 \\
			\vdots \\
			y_{m_2} \\
			w
		\end{pmatrix} & \ge 0
	\end{split} 
\end{equation}

\subsection{LP duality and zero-sum games}

We have the primal (P):

\begin{equation}
	\begin{split}
		\text{max}_{\vect{x}, v} v \text{ subject to }
		\vect{A}^\top \vect{x} & \ge \vect{v} \\
		\mathds{1}^\top \vect{x} & \le 1 \\
		\vect{x} & \ge 0 \\
		\vect{v} & \ge 0
	\end{split}
\end{equation}

We denote by $\vect{v}$ (and analogously for $\vect{w}$) the column vector
$\begin{pmatrix} v \\ \vdots \\ v \end{pmatrix}$. The dual (D) is:

\begin{equation}
	\begin{split}
		\text{min}_{\vect{y}, w} v \text{ subject to }
		\vect{A} \vect{y} & \le \vect{w} \\
		\mathds{1}^\top \vect{y} & \le 1 \\
		\vect{y} & \ge 0 \\
		\vect{w} & \ge 0
	\end{split}
\end{equation}

\begin{fact}
	\label{fact:A}
	For every (P) feasible solution $(\vect{x}, v)$ and for every
	mixed strategy $\vect{y} \in \Delta^{m_2}$, $(\vect{x}^\top
	\vect{A})\vect{y} \ge \vect{v}$.
\end{fact}

This says that the expected utility for player I by playing $\vect{x}$, if
player II chooses any $\vect{y}$, is at least $\vect{v}$.

\begin{proof}
	By the definition of the dot product:
	\begin{equation*}
		\vect{x}^\top \vect{A} \vect{y} = \sum_{j \in [m_2]}
		(\vect{x}^\top \vect{A})_j \cdot \vect{y}_j
	\end{equation*}

	By (P)-feasibility, $\vect{A}^\top \vect{x} \ge 0$ and
	$\vect{y}_j \ge 0$, so:
	\begin{equation*}
		\sum_{j \in [m_2]} (\vect{x}^\top \vect{A})_j \cdot
		\vect{y}_j \ge \sum_{j \in [m_2]} v \cdot \vect{y}_j = v
		\sum_{j \in [m_2]} \vect{y}_j = v
	\end{equation*}
\end{proof}

\begin{fact}
	\label{fact:B}
	If $\vect{A} \ge 0$, then for every (P)-optimal solution $(\vect{x}, v)$ we
	have that $\vect{x} \in \Delta^{m_1}$ is a mixed strategy for player I.
\end{fact}

\begin{proof}
	If $\mathds{1} \vect{x} > 1 - \varepsilon$ for some $\varepsilon > 0$, then
	$(\frac{\vect{x}}{1-\varepsilon}, \frac{v}{1-\varepsilon})$ is also
	(P)-feasible. TODO
\end{proof}

\begin{fact}
	\label{fact:C}
	For every (D)-feasible solution $(\vect{y}, w)$ and for every mixed
	strategy $\vect{x} \in \Delta^{m_1}$, $\vect{x}^\top(\vect{A} \vect{y}) \le
	w$.
\end{fact}

\begin{fact}
	\label{fact:D}
	If $\vect{A} \ge 0$, then for every (D)-optimal solution $(\vect{y}, w)$ we
	have that $\vect{y} \in \Delta^{m_2}$ is a mixed strategy for player II.
\end{fact}

\begin{fact}
	(P) has an optimal solution of finite value.
\end{fact}

\begin{proof}
	The feasible set is non-empty: TODO
\end{proof}

\begin{theorem}[Existence and structure of MNE in zero-sum games]
	For every two-player zero-sum game $\vect{A} \in \mathbb{R}^{m_1 \times
	m_2}_{\ge 0}$, there are mixed strategies $\vect{x} \in \Delta^{m_1},
	\vect{y} \in \Delta^{m_2}$ such that:

	\begin{itemize}
		\item $\vect{x}$ and $\vect{y}$ are maximin and minimax strategies,
			respectively
		\item $(\vect{x}, \vect{y})$ is a Nash Equilibrium
		\item all Nash Equilibria have the same expected payoffs (the
			\textnormal{value} of the game)
	\end{itemize}
\end{theorem}

\begin{proof}
	Let $(x, v)$ and $(y, w)$ be optimal solutions of (P) and (D),
	respectively.

	By the Strong Duality Theorem, we have that $v = w$. Using
	Facts~\ref{fact:A} to \ref{fact:D}, we know that $x$ is a maximin strategy
	for player I, and $y$ is a minimax strategy for player II.

	$x$ and $y$ are mutual best responses, so $(x,y)$ is a Nash Equilibrium.
\end{proof}

\begin{corollary}
	Equilibrium computation in two-player zero-sum games is computable in
	polynomial time.
\end{corollary}
